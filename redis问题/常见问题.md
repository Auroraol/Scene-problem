# Redis系列

# 缓存

## 缓存穿透 (不存在)

 现象: 

**指缓存和数据库都不存在对应的数据，导致所有的请求打到数据库上，容易把应用数据库击**垮。

解决方案:

+ 缓存空值：数据库不存在就缓存一个特殊的空值，并设置一个较短的国旗时间
+ 布隆过滤器：通过位图的方式检测缓存是否存在。下图为布隆过滤器示意图

![66B358FB-A5E7-438F-A3C5-F4E8D88B65E2.png](%E9%97%AE%E9%A2%98.assets/1623081984062-90d98bbf-477c-48fa-a5c6-b45cabf779ca.webp)

##  缓存击穿 (过期)

 现象: 

**热点缓存过期，多个线程同时并发访问热点缓存，由于缓存已过期，访问流量全部打到数据库，导致数据库负载过高甚至是宕机。**

解决方案:

+ 热点缓存永不过期，异步更新缓存
+ 通过互斥锁限制更新缓存的线程数量，此方式会导致吞吐量的下降，根据具体情况慎重选择

## 缓存雪崩 (过期+不存在)

现象: 

**大量缓存同时过期，或者大量缓存不存在且同时被访问，导致访问流量打到数据库，导致数据库负载过高甚至是宕机。**

解决方案 

+ 给缓存设置过期时间加一个随机值，避免大量缓存同一时间点过期
+ 缓存预热：做好缓存预热，避免初始时因大量缓存不存在而访问数据库
+ 当发生缓存雪崩时，通过限流，降级，熔断等手段避免雪崩范围进一步蔓延

#  Redis主从复制 

##  数据不一致问题 

主从数据不一致有以下原因：

 网络延迟 

 原因分析 

主从复制是异步的，因网络本身的不可靠性这种延迟是固有的。

 解决方案 

●从硬件环境考虑，给主从复制配置更好的网络环境

●监控主从复制延迟，比如通过 info replication命令通过比对 slave_repl_offset和master_repl_offset的值，当超过指定的阀值，就不允许访问从节点，降低延迟对业务的影响。

 过期数据 

redis对于过期数据的删除有两种策略：

●惰性删除：过期数据在访问时检测其是否过期，如果过期，会执行删除

●定时删除：redis定时扫描一定数量的key，对于过期的key执行数据删除



 访问过期数据 

在主从复制环境下，从节点上过期数据的删除是通过主节点传递删除的命令来删除的。从节点自己不会物理的删除过期数据。



访问主节点上的过期数据，主节点检测到过期，会进行删除，用户获取不到过期数据。



访问从节点上的过期数据，其行为和redis的版本有关。

在redis3.2之前，访问从节点上的过期数据，从节点不做检测，直接返回给用户。

在redis3.2之后，访问从节点上的过期数据，redis会检测数据是否过期，如果过期，则返回空。



 过期时间 

redis有四种设置缓存过期时间的命令，分别是

●expire 单位s, 相对时间

●pexpire 单位ms，相对时间

●expireat 绝对时间

●pexpireat 绝对时间

expire和pexpire 设置的过期时间是相对时间，redis开始处理expire或pexpire命令时的时间开始算起，expireat和pexpireat设置到是绝对时间，也就是时间点。



对于相对时间的设置方式，可能会由于网络原因或从节点负载的问题导致主节点数据已过期而从节点数据还未过期，这在上述所说的redis3.2之后的版本会导致主从数据不一致。



对于时间点这种设置方式，依赖于主从节点的时钟是否同步。



 解决方案 

1升级redis到3.2版本或更新版本，能够解决从节点过期数据可见性问题

2对于其他类型的（网络延迟，时钟不同步）只能是配置更好的网络环境以及时钟尽量同步



 安利 

解决从节点过期数据可见性问题（从节点不主动删除过期数据）还可以采用设置从节点可读写，但不推荐次方式。原因是当设置从节点可写，读取过期数据时确实会使得从节点删除过期数据，但如果在主节点给对应的key重新设置了过期时间，那么可能会导致主从节点数据不一致，从节点上已删除，主节点上未过期，同步到从节点时，从节点上已没有次数据。



从redis4.0开始，redis做了一个优化：发生在从节点上的过期数据(设置过期时间在从节点上)，redis会单独进行记录，惰性删除策略可以物理删除缓存数据。但从主节点上复制过来的数据依然不会物理删除。



 复制风暴 

复制风暴，是指redis 客户端缓冲区溢出和复制积压缓冲区溢出导致从节点不断的向主节点发起全量复制，给主节点CPU，内存，网络等带来巨大的消耗。



 客户端缓冲区溢出 



client-output-buffer-limit slave 256mb 64mb 60 

上述为客户端缓冲区默认配置项，slave 表示从节点类型的客户端， 后面三个值分别表示：最大限制，最小限制，最小限制持续时间。



此缓冲区可以理解为类似于socket buffer的功能，暂存发送到网络上的数据。当客户端缓冲区使用量超过最大限制或 达到最小限制且持续时间达到配置的值，主节点会直接关闭此连接，也就意味着复制失败。在哨兵模式下，哨兵节点会让从节点重新发起复制请求。



为什么会出现缓冲区溢出呢？快照数据量大，网络环境不好，从节点处理不及时，导致缓冲区不断的堆积。



 复制积压缓冲区 

![复制风暴-复制积压缓冲区.png](%E9%97%AE%E9%A2%98.assets/1605490359489-fb4860f1-8234-45f2-9f6f-55d39a2b57ec.webp)





复制积压缓冲区是指在主从之间出现短暂的网络断链，待复制数据被暂存到复制积压缓冲区。



复制积压缓冲区是环形区域，上图中master_repl_offset 表示在从节点发起全量复制或主从出现断链期间主节点写数据到复制积压缓冲区的位置。slave_repl_offset 为从节点需要从复制积压缓冲区中获取数据的起始位置。



当主从出现网络断链或数据集过大，且主节点写并发很高，就会导致master_repl_offset 追赶上slave_repl_offset，追赶上之后，slave_repl_offset和master_repl_offset 之间的数据便会被覆盖，当从节点请求其slave_repl_offset之后数据时无法从复制积压缓冲区中找到数据，便会触发从节点再次发起全量复制，以此往复，便出现复制风暴。



 复制风暴危害 

复制风暴之所以会对主节点产生比较大的影响，主要是因为主从之间的全量复制。

全量复制是从节点向主节点发起全量复制请求，主节点会fork一个子进程，生成RDB快照。



![复制风暴-fork内存页表.png](%E9%97%AE%E9%A2%98.assets/1605494158656-1bbd97ad-b09a-4e4b-b16a-adaceb9a156f.webp)



如果redis数据集很大，fork操作本身也会很耗时。由于fork使用COW(copy on write)写时复制技术，虽然fork操作使得子进程和父进程共享物理内存页，但是子进程要复制一些进程数据结构，其中内存页表（虚拟内存和物理内存地址映射表），redis占用内存越大，其页表就会越大，复制也就会更耗时。如果主节点写并发高，就会导致父进程将写操作对应的内存页进行拷贝，会占用额外内存，就导致redis内存占用增加，最终导致内存不足，且很可能会触发操作系统的OOM杀死redis进程。



除此之外，fork操作是在主线程中进行，fork耗时过长会阻塞主线程，那么主线程服务于外部的请求就会被阻塞。频繁的全量RDB对CPU资源消耗也会生高，会使得主线程对CPU的使用率降低。



频繁的全量复制，也会占用大量的网络带宽资源，如果redis节点是单网卡的话，那么外部请求的处理则会更耗时。



 复制风暴如何避免 

前面已经分析了复制风暴出现的原因及其危害。那么我们就可以采取针对性的措施来避免。



●根据网络环境及主从复制情况配置更大的客户端缓冲区和复制积压缓冲区

●避免给单redis节点配置过大的内存，一个大内存的节点不如多个小内存的节点（比如一个32G内存的节点，不如配置两个16G的节点）





目前业内的方案有两种
*(1)利用二级缓存*
比如利用ehcache，或者一个HashMap都可以。在你发现热key以后，把热key加载到系统的JVM中。
针对这种热key请求，会直接从jvm中取，而不会走到redis层。
假设此时有十万个针对同一个key的请求过来,如果没有本地缓存，这十万个请求就直接怼到同一台redis上了。
现在假设，你的应用层有50台机器，OK，你也有jvm缓存了。这十万个请求平均分散开来，每个机器有2000个请求，会从JVM中取到value值，然后返回数据。避免了十万个请求怼到同一台redis上的情形。
*(2)备份热key*
这个方案也很简单。不要让key走到同一台redis上不就行了。我们把这个key，在多个redis上都存一份不就好了。接下来，有热key请求进来的时候，我们就在有备份的redis上随机选取一台，进行访问取值，返回数据。



*(2)通知系统做处理*
在这个角度，有赞用的是上面的解决方案一:利用二级缓存进行处理。
有赞在监控到热key后，Hermes服务端集群会通过各种手段通知各业务系统里的Hermes-SDK，告诉他们:"老弟，这个key是热key，记得做本地缓存。"
于是Hermes-SDK就会将该key缓存在本地，对于后面的请求。Hermes-SDK发现这个是一个热key，直接从本地中拿，而不会去访问集群。

除了这种通知方式以外。我们也可以这么做，比如你的流式计算系统监控到热key了，往zookeeper里头的某个节点里写。然后你的业务系统监听该节点，发现节点数据变化了，就代表发现热key。最后往本地缓存里写，也是可以的。

通知方式各种各样，大家可以自由发挥。本文只是提供一个思路。